{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"Companies_with_Risk.xlsx\")\n",
        "\n",
        "df['FiscalDate'] = pd.to_datetime(df['FiscalDate'], errors='coerce')\n",
        "\n",
        "exclude_cols = ['GST', 'CropTax%', 'Inflation%', 'RepoRate%', 'USDINR_Close']\n",
        "\n",
        "def handle_outliers_within_company(group):\n",
        "    for col in group.select_dtypes(include=['float64', 'int64']).columns:\n",
        "        if col not in exclude_cols:\n",
        "            Q1 = group[col].quantile(0.25)\n",
        "            Q3 = group[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            group[col] = group[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "    return group\n",
        "\n",
        "# Apply outlier handling company-wise\n",
        "df = df.groupby(\"Company\", group_keys=False).apply(handle_outliers_within_company)\n",
        "\n",
        "# Drop exact duplicate rows (if any)\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Sorting\n",
        "df = df.sort_values(by=[\"Company\", \"FiscalDate\"], ascending=[True, True]).reset_index(drop=True)\n",
        "\n",
        "# Save cleaned dataset\n",
        "df.to_excel(\"Companies_with_Risk_Cleaned.xlsx\", index=False)\n",
        "print(\"Cleaned and sorted dataset saved as Companies_with_Risk_Cleaned.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgLHjazpl5e7",
        "outputId": "acaf2a61-4310-4f76-ad1b-58b4366fc79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3123634079.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(\"Company\", group_keys=False).apply(handle_outliers_within_company)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned and sorted dataset saved as Companies_with_Risk_Cleaned.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Ensure FiscalDate is datetime\n",
        "df[\"FiscalDate\"] = pd.to_datetime(df[\"FiscalDate\"])\n",
        "df = df.sort_values(by=[\"Company\", \"FiscalDate\"])\n",
        "\n",
        "# ==========================\n",
        "# Feature Engineering\n",
        "# ==========================\n",
        "df[\"Revenue_Growth\"] = df.groupby(\"Company\")[\"Revenue\"].pct_change()\n",
        "df[\"Expense_Growth\"] = df.groupby(\"Company\")[\"Expenses\"].pct_change()\n",
        "df[\"Profit_Growth\"] = df.groupby(\"Company\")[\"Net Profit\"].pct_change()\n",
        "df[\"Expense_to_Revenue\"] = df[\"Expenses\"] / df[\"Revenue\"]\n",
        "\n",
        "# Replace NaN with 0 (first row of each company)\n",
        "df[[\"Revenue_Growth\", \"Expense_Growth\", \"Profit_Growth\"]] = df[[\"Revenue_Growth\", \"Expense_Growth\", \"Profit_Growth\"]].fillna(0)\n",
        "\n",
        "# ==========================\n",
        "# Standardize (z-scores per company)\n",
        "# ==========================\n",
        "features = [\"Revenue_Growth\",\"Expense_Growth\",\"Profit_Growth\",\"Expense_to_Revenue\",\n",
        "            \"GST\",\"Inflation%\",\"RepoRate%\",\"USDINR_Close\"]\n",
        "\n",
        "for col in features:\n",
        "    df[col + \"_z\"] = df.groupby(\"Company\")[col].transform(\n",
        "        lambda x: (x - x.mean()) / x.std(ddof=0) if x.std(ddof=0) != 0 else 0\n",
        "    )\n",
        "\n",
        "# ==========================\n",
        "# Weighted Risk Score\n",
        "# ==========================\n",
        "df[\"Risk_Score\"] = (\n",
        "    -df[\"Revenue_Growth_z\"] +        # Lower revenue growth = higher risk\n",
        "     df[\"Expense_Growth_z\"] +        # Higher expense growth = higher risk\n",
        "    -df[\"Profit_Growth_z\"] +         # Lower profit growth = higher risk\n",
        "     df[\"Expense_to_Revenue_z\"] +    # Higher expense ratio = higher risk\n",
        "     df[\"GST_z\"] +\n",
        "     df[\"Inflation%_z\"] +\n",
        "     df[\"RepoRate%_z\"] +\n",
        "     df[\"USDINR_Close_z\"]\n",
        ")\n",
        "\n",
        "# ==========================\n",
        "# Classify Risk Levels\n",
        "# ==========================\n",
        "def classify_risk(x):\n",
        "    if x >= 1:   # Above 1 std deviation = stressed\n",
        "        return 2  # High risk\n",
        "    elif x <= -0.5:\n",
        "        return 0  # Low risk\n",
        "    else:\n",
        "        return 1  # Medium risk\n",
        "\n",
        "df[\"Risk\"] = df.groupby(\"Company\")[\"Risk_Score\"].transform(lambda x: x.apply(classify_risk))\n",
        "\n",
        "# ==========================\n",
        "# Save with new Risk column\n",
        "# ==========================\n",
        "output_path = \"/content/Companies_risklabel.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "LEZC8IS6dcbI",
        "outputId": "06167bc5-b73a-4b08-993e-3f83f9c26e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:52: RuntimeWarning: invalid value encountered in reduce\n",
            "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Companies_risklabel.xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/Companies_risklabel.xlsx\")\n",
        "df[\"FiscalDate\"] = pd.to_datetime(df[\"FiscalDate\"])\n",
        "df = df.sort_values([\"Company\", \"FiscalDate\"]).reset_index(drop=True)\n",
        "\n",
        "# Function to compute growth metrics\n",
        "def compute_growth(group):\n",
        "    group = group.sort_values(\"FiscalDate\").reset_index(drop=True)\n",
        "    group[\"Year\"] = group[\"FiscalDate\"].dt.year\n",
        "\n",
        "    freq = group[\"Year\"].value_counts().max()\n",
        "    is_quarterly = freq > 1\n",
        "\n",
        "    if is_quarterly:\n",
        "        # Quarterly companies\n",
        "        group[\"Revenue_QoQ_Growth\"] = group[\"Revenue\"].pct_change(1) * 100\n",
        "        group[\"Revenue_YoY_Growth\"] = group[\"Revenue\"].pct_change(4) * 100\n",
        "\n",
        "        if \"EPS\" in group.columns:\n",
        "            group[\"EPS_QoQ_Growth\"] = group[\"EPS\"].pct_change(1) * 100\n",
        "            group[\"EPS_YoY_Growth\"] = group[\"EPS\"].pct_change(4) * 100\n",
        "        else:\n",
        "            group[\"EPS_QoQ_Growth\"] = None\n",
        "            group[\"EPS_YoY_Growth\"] = None\n",
        "\n",
        "        group[\"Frequency\"] = \"Quarterly\"\n",
        "\n",
        "    else:\n",
        "        # Yearly companies\n",
        "        group[\"Revenue_QoQ_Growth\"] = 0\n",
        "        group[\"Revenue_YoY_Growth\"] = group[\"Revenue\"].pct_change(1) * 100\n",
        "        if \"EPS\" in group.columns:\n",
        "            group[\"EPS_QoQ_Growth\"] = 0\n",
        "            group[\"EPS_YoY_Growth\"] = group[\"EPS\"].pct_change(1) * 100\n",
        "        else:\n",
        "            group[\"EPS_QoQ_Growth\"] = 0\n",
        "            group[\"EPS_YoY_Growth\"] = None\n",
        "\n",
        "        group[\"Frequency\"] = \"Yearly\"\n",
        "\n",
        "    return group\n",
        "\n",
        "# Apply per company\n",
        "result = df.groupby(\"Company\", group_keys=False).apply(compute_growth)\n",
        "\n",
        "# --- Yearly Aggregation for Quarterly Companies ---\n",
        "yearly_agg = (\n",
        "    result[result[\"Frequency\"] == \"Quarterly\"]\n",
        "    .groupby([\"Company\", \"Year\"], as_index=False)\n",
        "    .agg({\n",
        "        \"Revenue\": \"sum\",\n",
        "        \"EPS\": \"sum\" if \"EPS\" in result.columns else \"mean\"\n",
        "    })\n",
        ")\n",
        "\n",
        "# Mark incomplete years (less than 4 quarters)\n",
        "counts = (\n",
        "    result[result[\"Frequency\"] == \"Quarterly\"]\n",
        "    .groupby([\"Company\", \"Year\"])[\"FiscalDate\"].count()\n",
        "    .reset_index(name=\"Quarters_Count\")\n",
        ")\n",
        "yearly_agg = yearly_agg.merge(counts, on=[\"Company\", \"Year\"], how=\"left\")\n",
        "yearly_agg[\"Incomplete_Year\"] = yearly_agg[\"Quarters_Count\"] < 4\n",
        "\n",
        "# Compute YoY growth on yearly totals\n",
        "yearly_agg[\"Revenue_YoY_Growth\"] = yearly_agg.groupby(\"Company\")[\"Revenue\"].pct_change(1) * 100\n",
        "if \"EPS\" in result.columns:\n",
        "    yearly_agg[\"EPS_YoY_Growth\"] = yearly_agg.groupby(\"Company\")[\"EPS\"].pct_change(1) * 100\n",
        "else:\n",
        "    yearly_agg[\"EPS_YoY_Growth\"] = None\n",
        "yearly_agg[\"Frequency\"] = \"Quarterly_Aggregated\"\n",
        "\n",
        "# --- Fill NaN values with 0 for ML ---\n",
        "for col in [\"Revenue_QoQ_Growth\", \"Revenue_YoY_Growth\", \"EPS_QoQ_Growth\", \"EPS_YoY_Growth\"]:\n",
        "    if col in result.columns:\n",
        "        result[col] = result[col].fillna(0)\n",
        "\n",
        "# Save outputs\n",
        "output_file = \"Companies_risklabel_with_growth.xlsx\"\n",
        "result.to_excel(output_file, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6UpFMUgtmsu",
        "outputId": "4b421f7b-0746-4786-a869-d7fc6a436e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3999056165.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  result = df.groupby(\"Company\", group_keys=False).apply(compute_growth)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/Companies_risklabel_with_growth.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "# 1. EBITDA Margin (%)\n",
        "df[\"EBITDA_Margin_%\"] = (df[\"EBITDA\"] / df[\"Revenue\"].replace(0, pd.NA)) * 100\n",
        "\n",
        "# 2. Net Profit Margin (%)\n",
        "df[\"Net_Profit_Margin_%\"] = (df[\"Net Profit\"] / df[\"Revenue\"].replace(0, pd.NA)) * 100\n",
        "\n",
        "# 3. Interest Coverage Ratio (x times)\n",
        "df[\"Interest_Coverage\"] = df[\"EBITDA\"] / df[\"Interest\"].replace(0, pd.NA)\n",
        "\n",
        "# 4. PBT / Interest Ratio (x times)\n",
        "df[\"PBT_Interest_Ratio\"] = df[\"PBT\"] / df[\"Interest\"].replace(0, pd.NA)\n",
        "\n",
        "# 5. Debt Proxy (Interest / Revenue) %\n",
        "df[\"Debt_Proxy_%\"] = (df[\"Interest\"] / df[\"Revenue\"].replace(0, pd.NA)) * 100\n",
        "\n",
        "# --- Save to new Excel file ---\n",
        "output_path = \"/content/Companies_with_Derived_Metrics.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "output_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IcV2Fj1KzEl4",
        "outputId": "396b222a-8bb5-4c0e-be9b-344f1092d8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Companies_with_Derived_Metrics.xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_updated = \"/content/Companies_with_Derived_Metrics.xlsx\"\n",
        "df = pd.read_excel(file_path_updated)\n",
        "df = df.sort_values(by=[\"Company\", \"FiscalDate\"])\n",
        "\n",
        "# 1. Stability Metrics (Rolling Volatility of QoQ Growth, 4 quarters)\n",
        "\n",
        "\n",
        "# Rolling volatility of Revenue QoQ Growth\n",
        "df[\"Revenue_QoQ_Volatility\"] = (\n",
        "    df.groupby(\"Company\")[\"Revenue_QoQ_Growth\"]\n",
        "      .rolling(window=4, min_periods=2)\n",
        "      .std()\n",
        "      .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "# Rolling volatility of EPS QoQ Growth\n",
        "df[\"EPS_QoQ_Volatility\"] = (\n",
        "    df.groupby(\"Company\")[\"EPS_QoQ_Growth\"]\n",
        "      .rolling(window=4, min_periods=2)\n",
        "      .std()\n",
        "      .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "\n",
        "# 2. Sector-relative Metrics\n",
        "\n",
        "\n",
        "# Median EBITDA Margin % by Sector & FiscalDate\n",
        "df[\"Sector_Median_EBITDA_Margin\"] = (\n",
        "    df.groupby([\"Sector\", \"FiscalDate\"])[\"EBITDA_Margin_%\"]\n",
        "      .transform(\"median\")\n",
        ")\n",
        "\n",
        "# Margin gap vs sector median\n",
        "df[\"EBITDA_Margin_Gap_vs_Sector\"] = (\n",
        "    df[\"EBITDA_Margin_%\"] - df[\"Sector_Median_EBITDA_Margin\"]\n",
        ")\n",
        "\n",
        "# Median Revenue QoQ Growth by Sector & FiscalDate\n",
        "df[\"Sector_Median_Revenue_QoQ_Growth\"] = (\n",
        "    df.groupby([\"Sector\", \"FiscalDate\"])[\"Revenue_QoQ_Growth\"]\n",
        "      .transform(\"median\")\n",
        ")\n",
        "\n",
        "# Revenue growth gap vs sector median\n",
        "df[\"Revenue_Growth_Gap_vs_Sector\"] = (\n",
        "    df[\"Revenue_QoQ_Growth\"] - df[\"Sector_Median_Revenue_QoQ_Growth\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Handle Missing Values\n",
        "\n",
        "df = df.fillna({\n",
        "    \"Revenue_QoQ_Volatility\": 0,\n",
        "    \"EPS_QoQ_Volatility\": 0,\n",
        "    \"Sector_Median_EBITDA_Margin\": 0,\n",
        "    \"EBITDA_Margin_Gap_vs_Sector\": 0,\n",
        "    \"Sector_Median_Revenue_QoQ_Growth\": 0,\n",
        "    \"Revenue_Growth_Gap_vs_Sector\": 0\n",
        "})\n",
        "\n",
        "\n",
        "# Save updated dataset\n",
        "\n",
        "output_path = \"/content/Companies_with_Stability_SectorMetrics.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "p4QBZh544Hps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1. Load dataset\n",
        "file_path = \"/content/Companies_with_Stability_SectorMetrics.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# 2. Define metric groups\n",
        "growth_metrics = [\"Revenue_QoQ_Growth\", \"Revenue_YoY_Growth\",\n",
        "                  \"EPS_QoQ_Growth\", \"EPS_YoY_Growth\"]\n",
        "\n",
        "profitability_metrics = [\"EBITDA_Margin_%\", \"Net_Profit_Margin_%\", \"Operating Margin %\"]\n",
        "\n",
        "risk_metrics = [\"Interest_Coverage\", \"PBT_Interest_Ratio\", \"Debt_Proxy_%\"]\n",
        "\n",
        "stability_metrics = [\"Revenue_QoQ_Volatility\", \"EPS_QoQ_Volatility\"]\n",
        "\n",
        "sector_metrics = [\"EBITDA_Margin_Gap_vs_Sector\", \"Revenue_Growth_Gap_vs_Sector\"]\n",
        "\n",
        "# 3. Data cleaning\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Cap extreme outliers for ratios\n",
        "for col in [\"Interest_Coverage\", \"PBT_Interest_Ratio\"]:\n",
        "    cap = df[col].quantile(0.99)\n",
        "    df[col] = np.where(df[col] > cap, cap, df[col])\n",
        "\n",
        "# 4. MinMax Scaling (0-100)\n",
        "all_metrics = growth_metrics + profitability_metrics + risk_metrics + stability_metrics + sector_metrics\n",
        "scaler = MinMaxScaler(feature_range=(0, 100))\n",
        "\n",
        "scaled_values = scaler.fit_transform(df[all_metrics])\n",
        "scaled_df = pd.DataFrame(scaled_values, columns=[m+\"_scaled\" for m in all_metrics], index=df.index)\n",
        "\n",
        "df = pd.concat([df, scaled_df], axis=1)\n",
        "\n",
        "# 5. Invert \"lower=better\" metrics\n",
        "for m in [\"Debt_Proxy_%\", \"Revenue_QoQ_Volatility\", \"EPS_QoQ_Volatility\"]:\n",
        "    df[m+\"_scaled_inv\"] = 100 - df[m+\"_scaled\"]\n",
        "\n",
        "# Update metric lists with scaled versions\n",
        "growth_scaled = [m+\"_scaled\" for m in growth_metrics]\n",
        "profitability_scaled = [m+\"_scaled\" for m in profitability_metrics]\n",
        "risk_scaled = [\"Interest_Coverage_scaled\", \"PBT_Interest_Ratio_scaled\", \"Debt_Proxy_%_scaled_inv\"]\n",
        "stability_scaled = [\"Revenue_QoQ_Volatility_scaled_inv\", \"EPS_QoQ_Volatility_scaled_inv\"]\n",
        "sector_scaled = [m+\"_scaled\" for m in sector_metrics]\n",
        "\n",
        "# 6. Composite scores (0-100 scale) using formulas\n",
        "\n",
        "# Growth Score (weights: QoQ > YoY)\n",
        "df[\"Growth_Score\"] = (\n",
        "    0.3 * df[\"Revenue_QoQ_Growth_scaled\"] +\n",
        "    0.2 * df[\"Revenue_YoY_Growth_scaled\"] +\n",
        "    0.3 * df[\"EPS_QoQ_Growth_scaled\"] +\n",
        "    0.2 * df[\"EPS_YoY_Growth_scaled\"]\n",
        ")\n",
        "\n",
        "# Profitability Score (weights: Net Profit > EBITDA > Operating)\n",
        "df[\"Profitability_Score\"] = (\n",
        "    0.35 * df[\"EBITDA_Margin_%_scaled\"] +\n",
        "    0.40 * df[\"Net_Profit_Margin_%_scaled\"] +\n",
        "    0.25 * df[\"Operating Margin %_scaled\"]\n",
        ")\n",
        "\n",
        "# Risk Score (weights: Debt Proxy > Interest Coverage > PBT/Interest)\n",
        "df[\"Risk_Score\"] = (\n",
        "    0.5 * df[\"Debt_Proxy_%_scaled_inv\"] +\n",
        "    0.3 * df[\"Interest_Coverage_scaled\"] +\n",
        "    0.2 * df[\"PBT_Interest_Ratio_scaled\"]\n",
        ")\n",
        "\n",
        "# Stability Score (equal weight)\n",
        "df[\"Stability_Score\"] = (\n",
        "    0.5 * df[\"Revenue_QoQ_Volatility_scaled_inv\"] +\n",
        "    0.5 * df[\"EPS_QoQ_Volatility_scaled_inv\"]\n",
        ")\n",
        "\n",
        "# Sector Score (equal weight)\n",
        "df[\"Sector_Score\"] = (\n",
        "    0.5 * df[\"EBITDA_Margin_Gap_vs_Sector_scaled\"] +\n",
        "    0.5 * df[\"Revenue_Growth_Gap_vs_Sector_scaled\"]\n",
        ")\n",
        "\n",
        "# Overall Score (equal weight of 5 groups)\n",
        "df[\"Overall_Score\"] = df[[\n",
        "    \"Growth_Score\", \"Profitability_Score\", \"Risk_Score\", \"Stability_Score\", \"Sector_Score\"\n",
        "]].mean(axis=1)\n",
        "\n",
        "\n",
        "# Overall score (equal weight of group scores)\n",
        "df[\"Overall_Score\"] = df[[\"Growth_Score\", \"Profitability_Score\",\n",
        "                          \"Risk_Score\", \"Stability_Score\", \"Sector_Score\"]].mean(axis=1)\n",
        "\n",
        "# Save output\n",
        "output_path = \"/content/Companies_with_Composite_Scores_0to100.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mox44pY56hcn",
        "outputId": "9c738927-1b4c-4bff-b836-21752cef2b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Companies_with_Composite_Scores_0to100.xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to drop\n",
        "drop_cols = [\n",
        "    \"Revenue_Growth\",\"Expense_Growth\",\"Profit_Growth\",\"Expense_to_Revenue\",\n",
        "    \"Revenue_Growth_z\",\"Expense_Growth_z\",\"Profit_Growth_z\",\"Expense_to_Revenue_z\",\n",
        "    \"GST_z\",\"Inflation%_z\",\"RepoRate%_z\",\"USDINR_Close_z\",\"Risk\",\n",
        "    \"Year\",\"Revenue_QoQ_Growth\",\"Revenue_YoY_Growth\",\"EPS_QoQ_Growth\",\"EPS_YoY_Growth\",\n",
        "    \"Frequency\",\"EBITDA_Margin_%\",\"Net_Profit_Margin_%\",\"Interest_Coverage\",\"PBT_Interest_Ratio\",\n",
        "    \"Debt_Proxy_%\",\"Revenue_QoQ_Volatility\",\"EPS_QoQ_Volatility\",\"Sector_Median_EBITDA_Margin\",\n",
        "    \"EBITDA_Margin_Gap_vs_Sector\",\"Sector_Median_Revenue_QoQ_Growth\",\"Revenue_Growth_Gap_vs_Sector\",\n",
        "    \"Revenue_QoQ_Growth_scaled\",\"Revenue_YoY_Growth_scaled\",\"EPS_QoQ_Growth_scaled\",\n",
        "    \"EPS_YoY_Growth_scaled\",\"EBITDA_Margin_%_scaled\",\"Net_Profit_Margin_%_scaled\",\n",
        "    \"Operating Margin %_scaled\",\"Interest_Coverage_scaled\",\"PBT_Interest_Ratio_scaled\",\n",
        "    \"Debt_Proxy_%_scaled\",\"Revenue_QoQ_Volatility_scaled\",\"EPS_QoQ_Volatility_scaled\",\n",
        "    \"EBITDA_Margin_Gap_vs_Sector_scaled\",\"Revenue_Growth_Gap_vs_Sector_scaled\",\n",
        "    \"Debt_Proxy_%_scaled_inv\",\"Revenue_QoQ_Volatility_scaled_inv\",\"EPS_QoQ_Volatility_scaled_inv\"\n",
        "]\n",
        "\n",
        "# Drop unwanted columns\n",
        "df_final = df.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "# Save to new Excel file\n",
        "final_output_path = \"/content/Companies_Final_Composite_Scores.xlsx\"\n",
        "df_final.to_excel(final_output_path, index=False)\n",
        "\n",
        "final_output_path\n"
      ],
      "metadata": {
        "id": "emOzn712Gkt-",
        "outputId": "9b423da1-d909-460f-c199-390b139979f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Companies_Final_Composite_Scores.xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}